apiVersion: apps/v1
kind: Deployment
metadata:
  name: ollama
spec:
  replicas: 1
  selector:
    matchLabels:
      deployment: ollama
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        deployment: ollama
    spec:
      containers:
      - name: serve
        image: ghcr.io/redhat-na-ssa/ollama:latest
        imagePullPolicy: Always
        command:
        - /bin/sh
        - -c
        - |
          #/bin/sh

          pull_model() {
              OLLAMA_HOST=localhost:11434
              MODEL_NAMES="granite3.3 all-minilm"

              for model in $MODEL_NAMES; do
                  until echo "curl -sL ${OLLAMA_HOST}/api/pull -d '{"name": "${model}"}'"; do
                      sleep 6
                  done
              done

          }

          serve_ollama(){
            /usr/local/bin/ollama serve
          }

          pull_model &
          serve_ollama

        ports:
        - containerPort: 11434
          protocol: TCP
        resources:
          limits:
            memory: 10Gi
        volumeMounts:
        - mountPath: /ollama
          name: storage
      restartPolicy: Always
      volumes:
      - name: storage
        persistentVolumeClaim:
          claimName: ollama
      tolerations:
      - key: "nvidia.com/gpu"
        operator: "Equal"
        effect: "NoSchedule"
        value: "true"
